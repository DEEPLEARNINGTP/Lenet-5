{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSqrrmWfSiQrhkchUzjROq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DEEPLEARNINGTP/Lenet-5/blob/main/TP9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Chemin du fichier ZIP\n",
        "zip_path = 'amhcd-data-64.zip'\n",
        "# Dossier de destination\n",
        "extract_folder = '/content'\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "# Extraction\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "print(f\"Dossier extrait dans : {extract_folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UntZ_oCr42b",
        "outputId": "626e7783-1148-4783-d89c-005d677a8ea2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dossier extrait dans : /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class LeNet5:\n",
        "    def __init__(self, learning_rate=0.001, num_classes=33):\n",
        "        \"\"\"\n",
        "        Initialisation du réseau LeNet-5\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Initialisation du cache\n",
        "        self.cache = {}\n",
        "\n",
        "        # Initialisation des poids et biais\n",
        "        self.initialize_parameters()\n",
        "\n",
        "        # Historique d'entraînement\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialisation des paramètres du réseau selon He initialization\n",
        "        \"\"\"\n",
        "        # Couche C1: 6 filtres 5x5x1\n",
        "        self.W1 = np.random.randn(6, 1, 5, 5) * np.sqrt(2.0 / (5*5*1))\n",
        "        self.b1 = np.zeros((6, 1))\n",
        "\n",
        "        # Couche C3: 16 filtres 5x5x6\n",
        "        self.W3 = np.random.randn(16, 6, 5, 5) * np.sqrt(2.0 / (5*5*6))\n",
        "        self.b3 = np.zeros((16, 1))\n",
        "\n",
        "        # Couche C5: 120 neurones (16*5*5 = 400 entrées)\n",
        "        self.W5 = np.random.randn(120, 400) * np.sqrt(2.0 / 400)\n",
        "        self.b5 = np.zeros((120, 1))\n",
        "\n",
        "        # Couche F6: 84 neurones\n",
        "        self.W6 = np.random.randn(84, 120) * np.sqrt(2.0 / 120)\n",
        "        self.b6 = np.zeros((84, 1))\n",
        "\n",
        "        # Couche de sortie: 33 neurones\n",
        "        self.W7 = np.random.randn(self.num_classes, 84) * np.sqrt(2.0 / 84)\n",
        "        self.b7 = np.zeros((self.num_classes, 1))\n",
        "\n",
        "    def tanh_activation(self, x):\n",
        "        \"\"\"Fonction d'activation tanh\"\"\"\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_derivative(self, x):\n",
        "        \"\"\"Dérivée de la fonction tanh\"\"\"\n",
        "        return 1 - np.tanh(x)**2\n",
        "\n",
        "    def sigmoid_activation(self, x):\n",
        "        \"\"\"Fonction d'activation sigmoïde\"\"\"\n",
        "        # Clip pour éviter overflow\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Dérivée de la fonction sigmoïde\"\"\"\n",
        "        s = self.sigmoid_activation(x)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"Fonction softmax pour la couche de sortie\"\"\"\n",
        "        # Stabilité numérique\n",
        "        x = x - np.max(x, axis=0, keepdims=True)\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
        "\n",
        "    def conv2d(self, input_data, kernel, bias, stride=1, padding=0):\n",
        "        \"\"\"\n",
        "        Convolution 2D\n",
        "        input_data: (batch_size, channels, height, width)\n",
        "        kernel: (out_channels, in_channels, kernel_height, kernel_width)\n",
        "        \"\"\"\n",
        "        batch_size, in_channels, in_h, in_w = input_data.shape\n",
        "        out_channels, _, k_h, k_w = kernel.shape\n",
        "\n",
        "        # Padding\n",
        "        if padding > 0:\n",
        "            input_data = np.pad(input_data, ((0, 0), (0, 0), (padding, padding), (padding, padding)), 'constant')\n",
        "            in_h += 2 * padding\n",
        "            in_w += 2 * padding\n",
        "\n",
        "        # Calcul des dimensions de sortie\n",
        "        out_h = (in_h - k_h) // stride + 1\n",
        "        out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "        # Initialisation de la sortie\n",
        "        output = np.zeros((batch_size, out_channels, out_h, out_w))\n",
        "\n",
        "        # Convolution\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * stride\n",
        "                        h_end = h_start + k_h\n",
        "                        w_start = j * stride\n",
        "                        w_end = w_start + k_w\n",
        "\n",
        "                        # Convolution sur toutes les couches d'entrée\n",
        "                        conv_sum = 0\n",
        "                        for ic in range(in_channels):\n",
        "                            conv_sum += np.sum(input_data[b, ic, h_start:h_end, w_start:w_end] * kernel[oc, ic])\n",
        "\n",
        "                        output[b, oc, i, j] = conv_sum + bias[oc, 0]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def avg_pool2d(self, input_data, pool_size=2, stride=2):\n",
        "        \"\"\"\n",
        "        Average Pooling 2D\n",
        "        \"\"\"\n",
        "        batch_size, channels, in_h, in_w = input_data.shape\n",
        "        out_h = (in_h - pool_size) // stride + 1\n",
        "        out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "        output = np.zeros((batch_size, channels, out_h, out_w))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * stride\n",
        "                        h_end = h_start + pool_size\n",
        "                        w_start = j * stride\n",
        "                        w_end = w_start + pool_size\n",
        "\n",
        "                        output[b, c, i, j] = np.mean(input_data[b, c, h_start:h_end, w_start:w_end])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Propagation avant\n",
        "        \"\"\"\n",
        "        # Initialisation du cache s'il n'existe pas\n",
        "        if not hasattr(self, 'cache'):\n",
        "            self.cache = {}\n",
        "\n",
        "        # Couche C1: Convolution + Tanh\n",
        "        self.cache['z1'] = self.conv2d(x, self.W1, self.b1)\n",
        "        self.cache['a1'] = self.tanh_activation(self.cache['z1'])\n",
        "\n",
        "        # Couche S2: Average Pooling\n",
        "        self.cache['a2'] = self.avg_pool2d(self.cache['a1'])\n",
        "\n",
        "        # Couche C3: Convolution + Tanh\n",
        "        self.cache['z3'] = self.conv2d(self.cache['a2'], self.W3, self.b3)\n",
        "        self.cache['a3'] = self.tanh_activation(self.cache['z3'])\n",
        "\n",
        "        # Couche S4: Average Pooling\n",
        "        self.cache['a4'] = self.avg_pool2d(self.cache['a3'])\n",
        "\n",
        "        # Flatten pour les couches fully connected\n",
        "        batch_size = x.shape[0]\n",
        "        self.cache['a4_flat'] = self.cache['a4'].reshape(batch_size, -1).T\n",
        "\n",
        "        # Couche C5: Fully Connected + Tanh\n",
        "        self.cache['z5'] = np.dot(self.W5, self.cache['a4_flat']) + self.b5\n",
        "        self.cache['a5'] = self.tanh_activation(self.cache['z5'])\n",
        "\n",
        "        # Couche F6: Fully Connected + Tanh\n",
        "        self.cache['z6'] = np.dot(self.W6, self.cache['a5']) + self.b6\n",
        "        self.cache['a6'] = self.tanh_activation(self.cache['z6'])\n",
        "\n",
        "        # Couche de sortie: Fully Connected + Softmax\n",
        "        self.cache['z7'] = np.dot(self.W7, self.cache['a6']) + self.b7\n",
        "        self.cache['a7'] = self.softmax(self.cache['z7'])\n",
        "\n",
        "        return self.cache['a7']\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Calcul de la perte cross-entropy\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[1]\n",
        "        # Éviter log(0)\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        loss = -np.sum(y_true * np.log(y_pred)) / batch_size\n",
        "        return loss\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Rétropropagation des gradients\n",
        "        \"\"\"\n",
        "        batch_size = y_true.shape[1]\n",
        "\n",
        "        # Gradient de la couche de sortie\n",
        "        dz7 = y_pred - y_true\n",
        "\n",
        "        # Gradients pour W7 et b7\n",
        "        dW7 = np.dot(dz7, self.cache['a6'].T) / batch_size\n",
        "        db7 = np.sum(dz7, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "        # Gradient pour la couche F6\n",
        "        da6 = np.dot(self.W7.T, dz7)\n",
        "        dz6 = da6 * self.tanh_derivative(self.cache['z6'])\n",
        "\n",
        "        # Gradients pour W6 et b6\n",
        "        dW6 = np.dot(dz6, self.cache['a5'].T) / batch_size\n",
        "        db6 = np.sum(dz6, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "        # Gradient pour la couche C5\n",
        "        da5 = np.dot(self.W6.T, dz6)\n",
        "        dz5 = da5 * self.tanh_derivative(self.cache['z5'])\n",
        "\n",
        "        # Gradients pour W5 et b5\n",
        "        dW5 = np.dot(dz5, self.cache['a4_flat'].T) / batch_size\n",
        "        db5 = np.sum(dz5, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "        # Reshape le gradient pour la couche S4\n",
        "        da4_flat = np.dot(self.W5.T, dz5)\n",
        "        da4 = da4_flat.T.reshape(self.cache['a4'].shape)\n",
        "\n",
        "        # Gradient pour la couche C3 (à travers S4)\n",
        "        da3 = self.avg_pool_backward(da4, self.cache['a3'].shape)\n",
        "        dz3 = da3 * self.tanh_derivative(self.cache['z3'])\n",
        "\n",
        "        # Gradients pour W3 et b3\n",
        "        dW3, db3 = self.conv_backward(dz3, self.cache['a2'], self.W3)\n",
        "\n",
        "        # Gradient pour la couche C1 (à travers S2)\n",
        "        da2 = self.conv_backward_input(dz3, self.W3, self.cache['a2'].shape)\n",
        "        da1 = self.avg_pool_backward(da2, self.cache['a1'].shape)\n",
        "        dz1 = da1 * self.tanh_derivative(self.cache['z1'])\n",
        "\n",
        "        # Gradients pour W1 et b1\n",
        "        dW1, db1 = self.conv_backward(dz1, self.cache['x'], self.W1)\n",
        "\n",
        "        # Stockage des gradients\n",
        "        self.grads = {\n",
        "            'dW7': dW7, 'db7': db7,\n",
        "            'dW6': dW6, 'db6': db6,\n",
        "            'dW5': dW5, 'db5': db5,\n",
        "            'dW3': dW3, 'db3': db3,\n",
        "            'dW1': dW1, 'db1': db1\n",
        "        }\n",
        "\n",
        "    def conv_backward(self, dout, x, w):\n",
        "        \"\"\"\n",
        "        Rétropropagation pour la convolution\n",
        "        \"\"\"\n",
        "        batch_size, out_channels, out_h, out_w = dout.shape\n",
        "        _, in_channels, in_h, in_w = x.shape\n",
        "        _, _, k_h, k_w = w.shape\n",
        "\n",
        "        dw = np.zeros_like(w)\n",
        "        db = np.zeros((out_channels, 1))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(out_channels):\n",
        "                db[oc] += np.sum(dout[b, oc])\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        for ic in range(in_channels):\n",
        "                            dw[oc, ic] += dout[b, oc, i, j] * x[b, ic, i:i+k_h, j:j+k_w]\n",
        "\n",
        "        return dw / batch_size, db / batch_size\n",
        "\n",
        "    def conv_backward_input(self, dout, w, input_shape):\n",
        "        \"\"\"\n",
        "        Rétropropagation pour l'entrée de la convolution\n",
        "        \"\"\"\n",
        "        batch_size, in_channels, in_h, in_w = input_shape\n",
        "        _, out_channels, out_h, out_w = dout.shape\n",
        "        _, _, k_h, k_w = w.shape\n",
        "\n",
        "        dx = np.zeros(input_shape)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for ic in range(in_channels):\n",
        "                for oc in range(out_channels):\n",
        "                    for i in range(out_h):\n",
        "                        for j in range(out_w):\n",
        "                            dx[b, ic, i:i+k_h, j:j+k_w] += dout[b, oc, i, j] * w[oc, ic]\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def avg_pool_backward(self, dout, input_shape):\n",
        "        \"\"\"\n",
        "        Rétropropagation pour l'average pooling\n",
        "        \"\"\"\n",
        "        batch_size, channels, in_h, in_w = input_shape\n",
        "        _, _, out_h, out_w = dout.shape\n",
        "\n",
        "        dx = np.zeros(input_shape)\n",
        "        pool_size = 2\n",
        "        stride = 2\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * stride\n",
        "                        h_end = h_start + pool_size\n",
        "                        w_start = j * stride\n",
        "                        w_end = w_start + pool_size\n",
        "\n",
        "                        # Distribuer le gradient uniformément\n",
        "                        dx[b, c, h_start:h_end, w_start:w_end] += dout[b, c, i, j] / (pool_size * pool_size)\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def update_parameters(self, optimizer='sgd'):\n",
        "        \"\"\"\n",
        "        Mise à jour des paramètres\n",
        "        \"\"\"\n",
        "        if optimizer == 'sgd':\n",
        "            self.W7 -= self.learning_rate * self.grads['dW7']\n",
        "            self.b7 -= self.learning_rate * self.grads['db7']\n",
        "            self.W6 -= self.learning_rate * self.grads['dW6']\n",
        "            self.b6 -= self.learning_rate * self.grads['db6']\n",
        "            self.W5 -= self.learning_rate * self.grads['dW5']\n",
        "            self.b5 -= self.learning_rate * self.grads['db5']\n",
        "            self.W3 -= self.learning_rate * self.grads['dW3']\n",
        "            self.b3 -= self.learning_rate * self.grads['db3']\n",
        "            self.W1 -= self.learning_rate * self.grads['dW1']\n",
        "            self.b1 -= self.learning_rate * self.grads['db1']\n",
        "        elif optimizer == 'adam':\n",
        "            self.adam_update()\n",
        "\n",
        "    def adam_update(self):\n",
        "        \"\"\"\n",
        "        Optimiseur Adam\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'adam_initialized'):\n",
        "            self.adam_initialized = True\n",
        "            self.beta1 = 0.9\n",
        "            self.beta2 = 0.999\n",
        "            self.epsilon = 1e-8\n",
        "            self.t = 0\n",
        "\n",
        "            # Initialisation des moments\n",
        "            self.m = {}\n",
        "            self.v = {}\n",
        "            for param in ['W7', 'b7', 'W6', 'b6', 'W5', 'b5', 'W3', 'b3', 'W1', 'b1']:\n",
        "                self.m[param] = np.zeros_like(getattr(self, param))\n",
        "                self.v[param] = np.zeros_like(getattr(self, param))\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        # Mise à jour des paramètres avec Adam\n",
        "        for param in ['W7', 'b7', 'W6', 'b6', 'W5', 'b5', 'W3', 'b3', 'W1', 'b1']:\n",
        "            grad = self.grads[f'd{param}']\n",
        "\n",
        "            # Mise à jour des moments\n",
        "            self.m[param] = self.beta1 * self.m[param] + (1 - self.beta1) * grad\n",
        "            self.v[param] = self.beta2 * self.v[param] + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "            # Correction du biais\n",
        "            m_corrected = self.m[param] / (1 - self.beta1 ** self.t)\n",
        "            v_corrected = self.v[param] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            # Mise à jour des paramètres\n",
        "            setattr(self, param, getattr(self, param) - self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon))\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, optimizer='adam'):\n",
        "        \"\"\"\n",
        "        Entraînement du modèle\n",
        "        \"\"\"\n",
        "        print(f\"Début de l'entraînement avec {len(X_train)} échantillons\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Mélange des données\n",
        "            indices = np.random.permutation(len(X_train))\n",
        "            X_train_shuffled = X_train[indices]\n",
        "            y_train_shuffled = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            epoch_correct = 0\n",
        "\n",
        "            # Entraînement par batch\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train_shuffled[i:i+batch_size]\n",
        "                batch_y = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Reshape pour le réseau\n",
        "                batch_X = batch_X.reshape(batch_X.shape[0], 1, 32, 32)\n",
        "\n",
        "                # Forward pass\n",
        "                self.cache['x'] = batch_X\n",
        "                y_pred = self.forward(batch_X)\n",
        "\n",
        "                # Calcul de la perte\n",
        "                loss = self.compute_loss(y_pred, batch_y.T)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                # Calcul de la précision\n",
        "                predictions = np.argmax(y_pred, axis=0)\n",
        "                true_labels = np.argmax(batch_y, axis=1)\n",
        "                epoch_correct += np.sum(predictions == true_labels)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(y_pred, batch_y.T)\n",
        "\n",
        "                # Mise à jour des paramètres\n",
        "                self.update_parameters(optimizer)\n",
        "\n",
        "            # Calcul des métriques d'entraînement\n",
        "            train_loss = epoch_loss / (len(X_train) // batch_size)\n",
        "            train_acc = epoch_correct / len(X_train)\n",
        "\n",
        "            # Évaluation sur l'ensemble de validation\n",
        "            val_loss, val_acc = self.evaluate(X_val, y_val)\n",
        "\n",
        "            # Stockage de l'historique\n",
        "            self.train_loss_history.append(train_loss)\n",
        "            self.val_loss_history.append(val_loss)\n",
        "            self.train_acc_history.append(train_acc)\n",
        "            self.val_acc_history.append(val_acc)\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"\n",
        "        Évaluation du modèle\n",
        "        \"\"\"\n",
        "        # Vérification que le cache existe\n",
        "        if not hasattr(self, 'cache'):\n",
        "            self.cache = {}\n",
        "\n",
        "        X = X.reshape(X.shape[0], 1, 32, 32)\n",
        "        y_pred = self.forward(X)\n",
        "\n",
        "        loss = self.compute_loss(y_pred, y.T)\n",
        "        predictions = np.argmax(y_pred, axis=0)\n",
        "        true_labels = np.argmax(y, axis=1)\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "\n",
        "        return loss, accuracy\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Prédiction\n",
        "        \"\"\"\n",
        "        # Vérification que le cache existe\n",
        "        if not hasattr(self, 'cache'):\n",
        "            self.cache = {}\n",
        "\n",
        "        X = X.reshape(X.shape[0], 1, 32, 32)\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=0)\n",
        "\n",
        "# Classe pour la gestion des données\n",
        "class TifinaghDataLoader:\n",
        "    def __init__(self, zip_path):\n",
        "        self.zip_path = zip_path\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Chargement des données depuis le fichier zip\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "\n",
        "            for file_name in file_list:\n",
        "                if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    # Extraction du label depuis le nom du fichier\n",
        "                    label = file_name.split('/')[-2] if '/' in file_name else file_name.split('_')[0]\n",
        "\n",
        "                    # Lecture de l'image\n",
        "                    with zip_ref.open(file_name) as file:\n",
        "                        img = Image.open(file).convert('L')  # Conversion en niveaux de gris\n",
        "                        img = img.resize((32, 32))  # Redimensionnement\n",
        "                        img_array = np.array(img) / 255.0  # Normalisation\n",
        "\n",
        "                        images.append(img_array)\n",
        "                        labels.append(label)\n",
        "\n",
        "        images = np.array(images)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Encodage des labels\n",
        "        labels_encoded = self.label_encoder.fit_transform(labels)\n",
        "\n",
        "        return images, labels_encoded\n",
        "\n",
        "    def one_hot_encode(self, labels, num_classes=33):\n",
        "        \"\"\"\n",
        "        Encodage one-hot des labels\n",
        "        \"\"\"\n",
        "        one_hot = np.zeros((len(labels), num_classes))\n",
        "        for i, label in enumerate(labels):\n",
        "            one_hot[i, label] = 1\n",
        "        return one_hot\n",
        "\n",
        "# Fonction de visualisation\n",
        "def plot_training_curves(model):\n",
        "    \"\"\"\n",
        "    Visualisation des courbes d'entraînement\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Courbes de perte\n",
        "    ax1.plot(model.train_loss_history, label='Train Loss')\n",
        "    ax1.plot(model.val_loss_history, label='Validation Loss')\n",
        "    ax1.set_title('Évolution de la Perte')\n",
        "    ax1.set_xlabel('Époque')\n",
        "    ax1.set_ylabel('Perte')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Courbes de précision\n",
        "    ax2.plot(model.train_acc_history, label='Train Accuracy')\n",
        "    ax2.plot(model.val_acc_history, label='Validation Accuracy')\n",
        "    ax2.set_title('Évolution de la Précision')\n",
        "    ax2.set_xlabel('Époque')\n",
        "    ax2.set_ylabel('Précision')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
        "    \"\"\"\n",
        "    Visualisation de la matrice de confusion\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Matrice de Confusion')\n",
        "    plt.ylabel('Vraie Classe')\n",
        "    plt.xlabel('Classe Prédite')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_feature_maps(model, sample_image):\n",
        "    \"\"\"\n",
        "    Visualisation des cartes de caractéristiques\n",
        "    \"\"\"\n",
        "    # Passage avant pour obtenir les activations\n",
        "    sample_image = sample_image.reshape(1, 1, 32, 32)\n",
        "    model.forward(sample_image)\n",
        "\n",
        "    # Visualisation des cartes de la couche C1\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    fig.suptitle('Cartes de Caractéristiques - Couche C1', fontsize=16)\n",
        "\n",
        "    for i in range(6):\n",
        "        ax = axes[i//3, i%3]\n",
        "        ax.imshow(model.cache['a1'][0, i], cmap='gray')\n",
        "        ax.set_title(f'Filtre {i+1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualisation des cartes de la couche C3\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "    fig.suptitle('Cartes de Caractéristiques - Couche C3', fontsize=16)\n",
        "\n",
        "    for i in range(16):\n",
        "        ax = axes[i//4, i%4]\n",
        "        ax.imshow(model.cache['a3'][0, i], cmap='gray')\n",
        "        ax.set_title(f'Filtre {i+1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Fonction principale\n",
        "def main():\n",
        "    # Chargement des données\n",
        "    print(\"Chargement des données...\")\n",
        "    data_loader = TifinaghDataLoader('amhcd-data-64.zip')\n",
        "    X, y = data_loader.load_data()\n",
        "\n",
        "    print(f\"Forme des données: {X.shape}\")\n",
        "    print(f\"Nombre de classes: {len(np.unique(y))}\")\n",
        "\n",
        "    # Encodage one-hot\n",
        "    y_one_hot = data_loader.one_hot_encode(y, num_classes=33)\n",
        "\n",
        "    # Division des données\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    print(f\"Taille d'entraînement: {X_train.shape[0]}\")\n",
        "    print(f\"Taille de validation: {X_val.shape[0]}\")\n",
        "    print(f\"Taille de test: {X_test.shape[0]}\")\n",
        "\n",
        "    # Initialisation du modèle\n",
        "    model = LeNet5(learning_rate=0.001, num_classes=33)\n",
        "\n",
        "    # Entraînement\n",
        "    print(\"\\nDébut de l'entraînement...\")\n",
        "    model.train(X_train, y_train, X_val, y_val, epochs=50, batch_size=32, optimizer='adam')\n",
        "\n",
        "    # Évaluation finale\n",
        "    print(\"\\nÉvaluation finale...\")\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "    print(f\"Précision sur l'ensemble de test: {test_acc:.4f}\")\n",
        "\n",
        "    # Visualisation des résultats\n",
        "    plot_training_curves(model)\n",
        "\n",
        "    # Prédictions pour la matrice de confusion\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Noms des classes (à adapter selon vos données)\n",
        "    class_names = [str(i) for i in range(33)]\n",
        "    plot_confusion_matrix(y_true, y_pred, class_names)\n",
        "\n",
        "    # Visualisation des cartes de caractéristiques\n",
        "    visualize_feature_maps(model, X_test[0])\n",
        "\n",
        "    # Rapport de classification\n",
        "    print(\"\\nRapport de classification:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Sauvegarde du modèle\n",
        "    with open('lenet5_tifinagh_model.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(\"Modèle sauvegardé dans 'lenet5_tifinagh_model.pkl'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "eo1VOMLq5GsC",
        "outputId": "1653dc43-a7b7-4ff1-ea5b-1b1e9c760442"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement des données...\n",
            "Forme des données: (28182, 32, 32)\n",
            "Nombre de classes: 33\n",
            "Taille d'entraînement: 19727\n",
            "Taille de validation: 4227\n",
            "Taille de test: 4228\n",
            "\n",
            "Début de l'entraînement...\n",
            "Début de l'entraînement avec 19727 échantillons\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-33397180.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-15-33397180.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;31m# Entraînement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDébut de l'entraînement...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;31m# Évaluation finale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-33397180.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, X_val, y_val, epochs, batch_size, optimizer)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Mise à jour des paramètres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-33397180.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# Gradient pour la couche C1 (à travers S2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mda2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_backward_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mda1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mdz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-33397180.py\u001b[0m in \u001b[0;36mconv_backward_input\u001b[0;34m(self, dout, w, input_shape)\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                             \u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk_w\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}